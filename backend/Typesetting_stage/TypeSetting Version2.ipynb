{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [\n",
    "    {\"x\": 413, \"y\": 105, \"w\": 49, \"h\": 55, \"text_ja\": \"あ!\", \"text_en\": \"Oh!\"},\n",
    "    {\"x\": 492, \"y\": 236, \"w\": 100, \"h\": 154, \"text_ja\": \"あれは一丁目のスナックのママ!\", \"text_en\": \"That woman is the hostess in the bar at Block-1.\"},\n",
    "    {\"x\": 91, \"y\": 244, \"w\": 94, \"h\": 119, \"text_ja\": \"あっちは行きつけの店の女将!\", \"text_en\": \"That is the owner of my favorite restaurant!\"},\n",
    "    {\"x\": 625, \"y\": 457, \"w\": 89, \"h\": 120, \"text_ja\": \"ワシもまだまだ人気者ですなぁ!\", \"text_en\": \"I'm still so popular!\"},\n",
    "    {\"x\": 540, \"y\": 529, \"w\": 71, \"h\": 141, \"text_ja\": \"生き生きしますぞ!\", \"text_en\": \"I feel so alive!\"},\n",
    "    {\"x\": 565, \"y\": 701, \"w\": 54, \"h\": 96, \"text_ja\": \"葬儀屋とは\", \"text_en\": \"The job of an undertaker\"},\n",
    "    {\"x\": 150, \"y\": 704, \"w\": 78, \"h\": 112, \"text_ja\": \"生者と死者の最期の場所を作る仕事\", \"text_en\": \"is to set up the last place for the living and the dead.\"},\n",
    "    {\"x\": 701, \"y\": 916, \"w\": 62, \"h\": 68, \"text_ja\": \"..もう\", \"text_en\": \"Well, I'm afraid...\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = torch.load(f\"boureisougi_masks.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_dict['boureisougi_002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'boureisougi_002.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked image saved to: boureisougi_002_masked.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Using your existing variables\n",
    "# mask_dict = torch.load(f\"boureisuogi_masks.pth\")\n",
    "# image_path = 'boureisuogi_002.jpg'\n",
    "\n",
    "# Get the mask for this specific image\n",
    "mask = mask_dict['boureisougi_002']\n",
    "\n",
    "# Open and convert the image\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Apply the mask (turn text pixels white)\n",
    "img_array[mask == 1] = [255, 255, 255]\n",
    "\n",
    "# Convert back to PIL Image\n",
    "masked_img = Image.fromarray(img_array)\n",
    "\n",
    "# Save the result\n",
    "output_path = image_path.replace('.jpg', '_masked.jpg')\n",
    "masked_img.save(output_path)\n",
    "\n",
    "print(f\"Masked image saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mask_dict.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Example inputs\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanga_page.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 169\u001b[0m     mask_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_dict.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Example bubble coordinates (x, y, w, h)\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mask_dict.pth'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def improved_text_masking(image_path, mask_tensor, bubble_coordinates, margin=5, dilation_size=2, output_path=None):\n",
    "    \"\"\"\n",
    "    Improved text masking that combines pixel-level mask with bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    bubble_coordinates : list of tuple\n",
    "        List of (x, y, w, h) coordinates for each speech bubble\n",
    "    margin : int\n",
    "        Extra margin to add around detected text regions\n",
    "    dilation_size : int\n",
    "        Size of dilation kernel to expand the text mask\n",
    "    output_path : str, optional\n",
    "        Path to save the masked image\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        The masked image with improved text removal\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "    \n",
    "    # Create a copy for the final output\n",
    "    final_img = img_array.copy()\n",
    "    \n",
    "    # Process each bubble\n",
    "    for bubble in bubble_coordinates:\n",
    "        x, y, w, h = bubble\n",
    "        \n",
    "        # Extract the bubble region from the mask\n",
    "        bubble_mask = mask[y:y+h, x:x+w]\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(bubble_mask) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Apply dilation to expand text regions and connect nearby text\n",
    "        kernel = np.ones((dilation_size, dilation_size), np.uint8)\n",
    "        dilated_mask = cv2.dilate(bubble_mask.astype(np.uint8), kernel, iterations=2)\n",
    "        \n",
    "        # Find connected components in the dilated mask\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_mask, connectivity=8)\n",
    "        \n",
    "        # Process each text component\n",
    "        for i in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component bounding box\n",
    "            cx = stats[i, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[i, cv2.CC_STAT_TOP]\n",
    "            cw = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Add margin\n",
    "            cx = max(0, cx - margin)\n",
    "            cy = max(0, cy - margin)\n",
    "            cw = min(w - cx, cw + 2 * margin)\n",
    "            ch = min(h - cy, ch + 2 * margin)\n",
    "            \n",
    "            # Create a rectangle mask for this component\n",
    "            component_mask = np.zeros_like(bubble_mask)\n",
    "            component_mask[cy:cy+ch, cx:cx+cw] = 1\n",
    "            \n",
    "            # Apply the component mask to the bubble region in the final image\n",
    "            # Convert local coordinates to global\n",
    "            global_mask = np.zeros_like(mask)\n",
    "            global_mask[y+cy:y+cy+ch, x+cx:x+cx+cw] = 1\n",
    "            \n",
    "            # Apply mask to image\n",
    "            final_img[global_mask == 1] = [255, 255, 255]\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    masked_img = Image.fromarray(final_img)\n",
    "    \n",
    "    # Save if output path provided\n",
    "    if output_path:\n",
    "        masked_img.save(output_path)\n",
    "        print(f\"Masked image saved to: {output_path}\")\n",
    "    \n",
    "    return masked_img\n",
    "\n",
    "def find_text_regions_in_bubbles(mask_tensor, bubble_coordinates, min_text_area=10):\n",
    "    \"\"\"\n",
    "    Find rectangular regions that contain text within each speech bubble.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    bubble_coordinates : list of tuple\n",
    "        List of (x, y, w, h) coordinates for each speech bubble\n",
    "    min_text_area : int\n",
    "        Minimum area for a text region to be considered\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuple\n",
    "        List of (bubble_index, x, y, w, h) for each detected text region\n",
    "    \"\"\"\n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "        \n",
    "    text_regions = []\n",
    "    \n",
    "    # Process each bubble\n",
    "    for i, bubble in enumerate(bubble_coordinates):\n",
    "        x, y, w, h = bubble\n",
    "        \n",
    "        # Extract the bubble region from the mask\n",
    "        bubble_mask = mask[y:y+h, x:x+w]\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(bubble_mask) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Apply dilation to connect nearby text\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        dilated_mask = cv2.dilate(bubble_mask.astype(np.uint8), kernel, iterations=1)\n",
    "        \n",
    "        # Find connected components in the dilated mask\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_mask, connectivity=8)\n",
    "        \n",
    "        # Process each text component\n",
    "        for j in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component stats\n",
    "            area = stats[j, cv2.CC_STAT_AREA]\n",
    "            \n",
    "            # Skip small noise\n",
    "            if area < min_text_area:\n",
    "                continue\n",
    "                \n",
    "            # Get component bounding box\n",
    "            cx = stats[j, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[j, cv2.CC_STAT_TOP]\n",
    "            cw = stats[j, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[j, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Convert to global coordinates\n",
    "            global_x = x + cx\n",
    "            global_y = y + cy\n",
    "            \n",
    "            # Add to text regions\n",
    "            text_regions.append((i, global_x, global_y, cw, ch))\n",
    "    \n",
    "    return text_regions\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example inputs\n",
    "    image_path = \"boureisougi_002.jpg\"\n",
    "    mask_dict = torch.load(f\"boureisougi_masks.pth\")\n",
    "    mask = mask_dict[\"boureisougi_002\"]\n",
    "    \n",
    "    # Example bubble coordinates (x, y, w, h)\n",
    "    bubble_coordinates = [\n",
    "        (100, 200, 150, 100),  # Example bubble 1\n",
    "        (300, 150, 200, 120),  # Example bubble 2\n",
    "    ]\n",
    "    \n",
    "    # Find text regions within bubbles\n",
    "    text_regions = find_text_regions_in_bubbles(mask, bubble_coordinates)\n",
    "    print(f\"Found {len(text_regions)} text regions\")\n",
    "    \n",
    "    # Apply improved masking\n",
    "    output_path = \"masked_manga.jpg\"\n",
    "    masked_img = improved_text_masking(image_path, mask, bubble_coordinates, margin=5, output_path=output_path)\n",
    "    \n",
    "    # Visualize the detected text regions (optional)\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    for i, x, y, w, h in text_regions:\n",
    "        # Draw rectangle around text region\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=\"red\")\n",
    "    \n",
    "    img.save(\"detected_text_regions.jpg\")\n",
    "    print(\"Text regions visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked image saved to: masked_manga.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def mask_text_with_bubbles(image_path, mask_tensor, input_data, margin=5, output_path=None):\n",
    "    \"\"\"\n",
    "    Mask text in manga using both pixel mask and speech bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of speech bubble data with keys 'x', 'y', 'w', 'h', 'text_ja', 'text_en'\n",
    "    margin : int\n",
    "        Extra margin to add around text regions\n",
    "    output_path : str, optional\n",
    "        Path to save the masked image\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        The masked image with text removed\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "    \n",
    "    # Create a drawing image for debugging\n",
    "    debug_img = img.copy()\n",
    "    draw = ImageDraw.Draw(debug_img)\n",
    "    \n",
    "    # Create output image\n",
    "    result_img = img_array.copy()\n",
    "    \n",
    "    # Process each bubble\n",
    "    for bubble in input_data:\n",
    "        x, y, w, h = bubble[\"x\"], bubble[\"y\"], bubble[\"w\"], bubble[\"h\"]\n",
    "        \n",
    "        # Draw original bubble outline\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=\"blue\", width=1)\n",
    "        \n",
    "        # Create a mask for this bubble region\n",
    "        bubble_region = np.zeros_like(mask)\n",
    "        bubble_region[y:y+h, x:x+w] = 1\n",
    "        \n",
    "        # Get the text mask within this bubble\n",
    "        text_in_bubble = np.logical_and(mask == 1, bubble_region == 1)\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(text_in_bubble) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find connected components (text clusters)\n",
    "        text_img = text_in_bubble.astype(np.uint8) * 255\n",
    "        \n",
    "        # Apply dilation to connect nearby text\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        dilated_text = cv2.dilate(text_img, kernel, iterations=1)\n",
    "        \n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_text, connectivity=8)\n",
    "        \n",
    "        # Process each text component\n",
    "        for i in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component bounding box\n",
    "            cx = stats[i, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[i, cv2.CC_STAT_TOP]\n",
    "            cw = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Expand with margin\n",
    "            cx_expanded = max(0, cx - margin)\n",
    "            cy_expanded = max(0, cy - margin)\n",
    "            cw_expanded = cw + 2 * margin\n",
    "            ch_expanded = ch + 2 * margin\n",
    "            \n",
    "            # Draw the expanded text region\n",
    "            draw.rectangle([cx_expanded, cy_expanded, cx_expanded+cw_expanded, cy_expanded+ch_expanded], \n",
    "                          outline=\"red\", width=1)\n",
    "            \n",
    "            # Create a mask for the expanded text region\n",
    "            text_region = np.zeros_like(mask)\n",
    "            text_region[cy_expanded:cy_expanded+ch_expanded, cx_expanded:cx_expanded+cw_expanded] = 1\n",
    "            \n",
    "            # Apply white color to the text region\n",
    "            result_img[text_region == 1] = [255, 255, 255]\n",
    "    \n",
    "    # Save debug image\n",
    "    debug_img.save(image_path.replace('.jpg', '_debug.jpg'))\n",
    "    \n",
    "    # Create output image\n",
    "    final_img = Image.fromarray(result_img)\n",
    "    \n",
    "    if output_path:\n",
    "        final_img.save(output_path)\n",
    "        print(f\"Masked image saved to: {output_path}\")\n",
    "    \n",
    "    return final_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your variables\n",
    "    image_path = \"boureisougi_002.jpg\"\n",
    "    mask_dict = torch.load(f\"boureisougi_masks.pth\")\n",
    "    mask = mask_dict[\"boureisougi_002\"]\n",
    "    \n",
    "    # Your input data\n",
    "    input_data = [\n",
    "        {\"x\": 413, \"y\": 105, \"w\": 49, \"h\": 55, \"text_ja\": \"あ!\", \"text_en\": \"Oh!\"},\n",
    "        {\"x\": 492, \"y\": 236, \"w\": 100, \"h\": 154, \"text_ja\": \"あれは一丁目のスナックのママ!\", \"text_en\": \"That woman is the hostess in the bar at Block-1.\"},\n",
    "        {\"x\": 91, \"y\": 244, \"w\": 94, \"h\": 119, \"text_ja\": \"あっちは行きつけの店の女将!\", \"text_en\": \"That is the owner of my favorite restaurant!\"},\n",
    "        {\"x\": 625, \"y\": 457, \"w\": 89, \"h\": 120, \"text_ja\": \"ワシもまだまだ人気者ですなぁ!\", \"text_en\": \"I'm still so popular!\"},\n",
    "        {\"x\": 540, \"y\": 529, \"w\": 71, \"h\": 141, \"text_ja\": \"生き生きしますぞ!\", \"text_en\": \"I feel so alive!\"},\n",
    "        {\"x\": 565, \"y\": 701, \"w\": 54, \"h\": 96, \"text_ja\": \"葬儀屋とは\", \"text_en\": \"The job of an undertaker\"},\n",
    "        {\"x\": 150, \"y\": 704, \"w\": 78, \"h\": 112, \"text_ja\": \"生者と死者の最期の場所を作る仕事\", \"text_en\": \"is to set up the last place for the living and the dead.\"},\n",
    "        {\"x\": 701, \"y\": 916, \"w\": 62, \"h\": 68, \"text_ja\": \"..もう\", \"text_en\": \"Well, I'm afraid...\"},\n",
    "    ]\n",
    "    \n",
    "    # Apply masking\n",
    "    output_path = \"masked_manga.jpg\"\n",
    "    masked_img = mask_text_with_bubbles(image_path, mask, input_data, margin=5, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x1 must be greater than or equal to x0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 368\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Translate the manga page\u001b[39;00m\n\u001b[1;32m    367\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboureisougi_002_translated.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 368\u001b[0m translated_img \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_manga_page\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    374\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 328\u001b[0m, in \u001b[0;36mtranslate_manga_page\u001b[0;34m(image_path, mask_tensor, input_data, font_path, output_path, margin, base_font_size, debug)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    327\u001b[0m             x, y, w, h \u001b[38;5;241m=\u001b[39m region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m], region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 328\u001b[0m             \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrectangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     debug_img\u001b[38;5;241m.\u001b[39msave(image_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_regions.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# Step 2: Add translated text\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/PIL/ImageDraw.py:305\u001b[0m, in \u001b[0;36mImageDraw.rectangle\u001b[0;34m(self, xy, fill, outline, width)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mdraw_rectangle(xy, fill, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ink \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ink \u001b[38;5;241m!=\u001b[39m fill \u001b[38;5;129;01mand\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_rectangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: x1 must be greater than or equal to x0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def mask_text_and_get_regions(image_path, mask_tensor, input_data, margin=5):\n",
    "    \"\"\"\n",
    "    Mask text in manga and return the optimized text regions for each bubble.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of speech bubble data with keys 'x', 'y', 'w', 'h', 'text_ja', 'text_en'\n",
    "    margin : int\n",
    "        Extra margin to add around text regions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (masked_image, optimized_regions)\n",
    "        - masked_image: PIL.Image with text removed\n",
    "        - optimized_regions: list of dicts with optimized text placement info\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "    \n",
    "    # Create output image\n",
    "    result_img = img_array.copy()\n",
    "    \n",
    "    # Create optimized text regions list\n",
    "    optimized_regions = []\n",
    "    \n",
    "    # Process each bubble\n",
    "    for i, bubble in enumerate(input_data):\n",
    "        x, y, w, h = bubble[\"x\"], bubble[\"y\"], bubble[\"w\"], bubble[\"h\"]\n",
    "        \n",
    "        # Create a mask for this bubble region\n",
    "        bubble_region = np.zeros_like(mask)\n",
    "        bubble_region[y:y+h, x:x+w] = 1\n",
    "        \n",
    "        # Get the text mask within this bubble\n",
    "        text_in_bubble = np.logical_and(mask == 1, bubble_region == 1)\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(text_in_bubble) == 0:\n",
    "            # Still add the original bubble to optimized regions\n",
    "            optimized_regions.append({\n",
    "                \"original_index\": i,\n",
    "                \"x\": x, \n",
    "                \"y\": y, \n",
    "                \"w\": w, \n",
    "                \"h\": h,\n",
    "                \"text_en\": bubble[\"text_en\"],\n",
    "                \"optimized\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Find connected components (text clusters)\n",
    "        text_img = text_in_bubble.astype(np.uint8) * 255\n",
    "        \n",
    "        # Apply dilation to connect nearby text\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        dilated_text = cv2.dilate(text_img, kernel, iterations=1)\n",
    "        \n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_text, connectivity=8)\n",
    "        \n",
    "        # Skip if no significant components found\n",
    "        if num_labels <= 1:\n",
    "            optimized_regions.append({\n",
    "                \"original_index\": i,\n",
    "                \"x\": x, \n",
    "                \"y\": y, \n",
    "                \"w\": w, \n",
    "                \"h\": h,\n",
    "                \"text_en\": bubble[\"text_en\"],\n",
    "                \"optimized\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get the bounding box that contains all text components\n",
    "        min_x = float('inf')\n",
    "        min_y = float('inf')\n",
    "        max_x = 0\n",
    "        max_y = 0\n",
    "        \n",
    "        for j in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component bounding box\n",
    "            cx = stats[j, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[j, cv2.CC_STAT_TOP]\n",
    "            cw = stats[j, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[j, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Update min/max coordinates\n",
    "            min_x = min(min_x, cx)\n",
    "            min_y = min(min_y, cy)\n",
    "            max_x = max(max_x, cx + cw)\n",
    "            max_y = max(max_y, cy + ch)\n",
    "            \n",
    "            # Apply white color to this component with margin\n",
    "            component_mask = np.zeros_like(mask)\n",
    "            \n",
    "            # Add margin\n",
    "            cx_expanded = max(0, cx - margin)\n",
    "            cy_expanded = max(0, cy - margin)\n",
    "            cw_expanded = cw + 2 * margin\n",
    "            ch_expanded = ch + 2 * margin\n",
    "            \n",
    "            # Apply expanded component mask\n",
    "            component_mask[cy_expanded:cy_expanded+ch_expanded, cx_expanded:cx_expanded+cw_expanded] = 1\n",
    "            \n",
    "            # Apply white to result image\n",
    "            result_img[component_mask == 1] = [255, 255, 255]\n",
    "        \n",
    "        # Calculate the optimized text region coordinates\n",
    "        opt_x = x + min_x\n",
    "        opt_y = y + min_y\n",
    "        opt_w = max_x - min_x\n",
    "        opt_h = max_y - min_y\n",
    "        \n",
    "        # Add margins to optimized region\n",
    "        opt_x = max(x, opt_x - margin)\n",
    "        opt_y = max(y, opt_y - margin)\n",
    "        opt_w = min(w - (opt_x - x), opt_w + 2 * margin)\n",
    "        opt_h = min(h - (opt_y - y), opt_h + 2 * margin)\n",
    "        \n",
    "        # Add optimized region to list\n",
    "        optimized_regions.append({\n",
    "            \"original_index\": i,\n",
    "            \"x\": opt_x, \n",
    "            \"y\": opt_y, \n",
    "            \"w\": opt_w, \n",
    "            \"h\": opt_h,\n",
    "            \"text_en\": bubble[\"text_en\"],\n",
    "            \"optimized\": True\n",
    "        })\n",
    "    \n",
    "    # Create final masked image\n",
    "    masked_img = Image.fromarray(result_img)\n",
    "    \n",
    "    return masked_img, optimized_regions\n",
    "\n",
    "def format_text_for_bubble(text, max_width, max_height, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    Format text to fit within a bubble with line wrapping and font size adjustment.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(Image.new('RGB', (1, 1)))\n",
    "    font = ImageFont.truetype(font_path, base_size)\n",
    "    \n",
    "    # 1. First attempt simple text wrapping\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    \n",
    "    for word in words:\n",
    "        test_line = ' '.join(current_line + [word])\n",
    "        bbox = draw.textbbox((0, 0), test_line, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        \n",
    "        if text_width <= max_width:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            if current_line:\n",
    "                lines.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "            else:\n",
    "                # If the word is too long, consider breaking the word or adjusting the font size\n",
    "                current_line = [word]\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    \n",
    "    # 2. Check if the total height fits\n",
    "    total_height = 0\n",
    "    line_spacing = base_size * 0.3  # 30% line spacing\n",
    "    \n",
    "    for line in lines:\n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "\n",
    "    # 3. If height exceeds, consider reducing font size appropriately\n",
    "    if total_height > max_height:\n",
    "        # Reduce size proportionally based on overflow ratio\n",
    "        ratio = max_height / total_height\n",
    "        new_size = max(int(base_size * ratio * 0.95), 10)  # Minimum size of 10\n",
    "        return format_text_for_bubble(text, max_width, max_height, font_path, new_size)\n",
    "    \n",
    "    return lines, base_size\n",
    "\n",
    "def add_translated_text(image, text_regions, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    Add translated text to the masked image.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : PIL.Image\n",
    "        Masked image\n",
    "    text_regions : list of dict\n",
    "        List of optimized text regions\n",
    "    font_path : str\n",
    "        Path to font file\n",
    "    base_size : int\n",
    "        Base font size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Image with translated text added\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    for region in text_regions:\n",
    "        x, y = region[\"x\"], region[\"y\"]\n",
    "        w, h = region[\"w\"], region[\"h\"]\n",
    "        text = region[\"text_en\"]\n",
    "        \n",
    "        # Format text to fit in this region\n",
    "        lines, final_size = format_text_for_bubble(\n",
    "            text,\n",
    "            w * 0.9,  # 90% of region width\n",
    "            h * 0.9,  # 90% of region height\n",
    "            font_path,\n",
    "            base_size\n",
    "        )\n",
    "        \n",
    "        # Load the font at the final determined size\n",
    "        font = ImageFont.truetype(font_path, final_size)\n",
    "        \n",
    "        # Calculate total text height (for vertical centering)\n",
    "        total_height = 0\n",
    "        line_spacing = final_size * 0.3\n",
    "        for line in lines:\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "        total_height -= line_spacing  # Subtract the line spacing added for the last line\n",
    "        \n",
    "        # Calculate the starting y-coordinate to vertically center the text\n",
    "        start_y = y + (h - total_height) // 2\n",
    "        \n",
    "        # Draw each line of text\n",
    "        current_y = start_y\n",
    "        for line in lines:\n",
    "            # Get the width of the current line for horizontal centering\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            text_width = bbox[2] - bbox[0]\n",
    "            text_x = x + (w - text_width) // 2\n",
    "            \n",
    "            # Draw text (with outline)\n",
    "            draw.text(\n",
    "                (text_x, current_y),\n",
    "                line,\n",
    "                font=font,\n",
    "                fill='black',\n",
    "                stroke_width=2,\n",
    "                stroke_fill='white'\n",
    "            )\n",
    "            \n",
    "            # Update the y-coordinate for the next line\n",
    "            current_y += (bbox[3] - bbox[1]) + line_spacing\n",
    "    \n",
    "    return image\n",
    "\n",
    "def translate_manga_page(image_path, mask_tensor, input_data, font_path, output_path=None, \n",
    "                         margin=5, base_font_size=22, debug=False):\n",
    "    \"\"\"\n",
    "    Complete manga translation process:\n",
    "    1. Mask original text\n",
    "    2. Find optimized text regions\n",
    "    3. Add translated text\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of speech bubble data\n",
    "    font_path : str\n",
    "        Path to font file\n",
    "    output_path : str, optional\n",
    "        Path to save the translated image\n",
    "    margin : int\n",
    "        Margin to add around text regions\n",
    "    base_font_size : int\n",
    "        Base font size for text\n",
    "    debug : bool\n",
    "        If True, save debug images\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Translated image\n",
    "    \"\"\"\n",
    "    # Step 1: Mask text and get optimized regions\n",
    "    masked_img, optimized_regions = mask_text_and_get_regions(\n",
    "        image_path, mask_tensor, input_data, margin\n",
    "    )\n",
    "    \n",
    "    # Save masked image for debugging\n",
    "    if debug:\n",
    "        masked_img.save(image_path.replace('.jpg', '_masked.jpg'))\n",
    "        \n",
    "        # Create debug image showing regions\n",
    "        debug_img = masked_img.copy()\n",
    "        draw = ImageDraw.Draw(debug_img)\n",
    "        \n",
    "        # Draw original bubbles in blue\n",
    "        for bubble in input_data:\n",
    "            x, y, w, h = bubble[\"x\"], bubble[\"y\"], bubble[\"w\"], bubble[\"h\"]\n",
    "            draw.rectangle([x, y, x+w, y+h], outline=\"blue\", width=1)\n",
    "        \n",
    "        # Draw optimized regions in red\n",
    "        for region in optimized_regions:\n",
    "            if region[\"optimized\"]:\n",
    "                x, y, w, h = region[\"x\"], region[\"y\"], region[\"w\"], region[\"h\"]\n",
    "                draw.rectangle([x, y, x+w, y+h], outline=\"red\", width=1)\n",
    "        \n",
    "        debug_img.save(image_path.replace('.jpg', '_regions.jpg'))\n",
    "    \n",
    "    # Step 2: Add translated text\n",
    "    translated_img = add_translated_text(\n",
    "        masked_img, optimized_regions, font_path, base_font_size\n",
    "    )\n",
    "    \n",
    "    # Save translated image\n",
    "    if output_path:\n",
    "        translated_img.save(output_path)\n",
    "        print(f\"Translated image saved to: {output_path}\")\n",
    "    \n",
    "    return translated_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your exact variables\n",
    "    image_path = \"boureisougi_002.jpg\"\n",
    "    mask_dict = torch.load(f\"boureisougi_masks.pth\")\n",
    "    mask = mask_dict[\"boureisougi_002\"]\n",
    "    \n",
    "    # Your input data\n",
    "    input_data = [\n",
    "        {\"x\": 413, \"y\": 105, \"w\": 49, \"h\": 55, \"text_ja\": \"あ!\", \"text_en\": \"Oh!\"},\n",
    "        {\"x\": 492, \"y\": 236, \"w\": 100, \"h\": 154, \"text_ja\": \"あれは一丁目のスナックのママ!\", \"text_en\": \"That woman is the hostess in the bar at Block-1.\"},\n",
    "        {\"x\": 91, \"y\": 244, \"w\": 94, \"h\": 119, \"text_ja\": \"あっちは行きつけの店の女将!\", \"text_en\": \"That is the owner of my favorite restaurant!\"},\n",
    "        {\"x\": 625, \"y\": 457, \"w\": 89, \"h\": 120, \"text_ja\": \"ワシもまだまだ人気者ですなぁ!\", \"text_en\": \"I'm still so popular!\"},\n",
    "        {\"x\": 540, \"y\": 529, \"w\": 71, \"h\": 141, \"text_ja\": \"生き生きしますぞ!\", \"text_en\": \"I feel so alive!\"},\n",
    "        {\"x\": 565, \"y\": 701, \"w\": 54, \"h\": 96, \"text_ja\": \"葬儀屋とは\", \"text_en\": \"The job of an undertaker\"},\n",
    "        {\"x\": 150, \"y\": 704, \"w\": 78, \"h\": 112, \"text_ja\": \"生者と死者の最期の場所を作る仕事\", \"text_en\": \"is to set up the last place for the living and the dead.\"},\n",
    "        {\"x\": 701, \"y\": 916, \"w\": 62, \"h\": 68, \"text_ja\": \"..もう\", \"text_en\": \"Well, I'm afraid...\"},\n",
    "    ]\n",
    "    \n",
    "    # Font path\n",
    "    font_path = \"/System/Library/Fonts/Supplemental/Arial.ttf\"\n",
    "    \n",
    "    # Translate the manga page\n",
    "    output_path = \"boureisougi_002_translated.jpg\"\n",
    "    translated_img = translate_manga_page(\n",
    "        image_path, mask, input_data, font_path, \n",
    "        output_path=output_path, \n",
    "        margin=5, \n",
    "        base_font_size=22,\n",
    "        debug=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked image saved to: boureisougi_002_masked.jpg\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 301\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Translate the manga page\u001b[39;00m\n\u001b[1;32m    300\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboureisougi_002_translated.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 301\u001b[0m translated_img \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_manga_page\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22\u001b[39;49m\n\u001b[1;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 266\u001b[0m, in \u001b[0;36mtranslate_manga_page\u001b[0;34m(image_path, mask_tensor, input_data, font_path, output_path, margin, base_font_size)\u001b[0m\n\u001b[1;32m    260\u001b[0m masked_img \u001b[38;5;241m=\u001b[39m mask_text_with_bubbles(\n\u001b[1;32m    261\u001b[0m     image_path, mask_tensor, input_data, margin, \n\u001b[1;32m    262\u001b[0m     output_path\u001b[38;5;241m=\u001b[39mimage_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_masked.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Step 2: Add translated text using original bubble coordinates\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m translated_img \u001b[38;5;241m=\u001b[39m \u001b[43madd_translated_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_font_size\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Save the translated image\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path:\n",
      "Cell \u001b[0;32mIn[30], line 186\u001b[0m, in \u001b[0;36madd_translated_text\u001b[0;34m(image, input_data, font_path, base_size)\u001b[0m\n\u001b[1;32m    183\u001b[0m text_en \u001b[38;5;241m=\u001b[39m bubble[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_en\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Format text to fit in the bubble\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m lines, final_size \u001b[38;5;241m=\u001b[39m \u001b[43mformat_text_for_bubble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_en\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 90% of bubble width (10% margin)\u001b[39;49;00m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 90% of bubble height (10% margin)\u001b[39;49;00m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_size\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Load the font at the final determined size\u001b[39;00m\n\u001b[1;32m    195\u001b[0m font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mtruetype(font_path, final_size)\n",
      "Cell \u001b[0;32mIn[30], line 115\u001b[0m, in \u001b[0;36mformat_text_for_bubble\u001b[0;34m(text, max_width, max_height, font_path, base_size)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mFirst try to wrap text, then consider adjusting font size.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m draw \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mDraw(Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m--> 115\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[43mImageFont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# 1. First attempt simple text wrapping\u001b[39;00m\n\u001b[1;32m    118\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/PIL/ImageFont.py:819\u001b[0m, in \u001b[0;36mtruetype\u001b[0;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfreetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/PIL/ImageFont.py:816\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[0;34m(font)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreetype\u001b[39m(font):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFreeTypeFont\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/conda/envs/t_ext/lib/python3.9/site-packages/PIL/ImageFont.py:245\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[0;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 load_from_bytes(f)\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfont \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfont\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayout_engine\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     load_from_bytes(font)\n",
      "\u001b[0;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def mask_text_with_bubbles(image_path, mask_tensor, input_data, margin=5, output_path=None):\n",
    "    \"\"\"\n",
    "    Mask text in manga using both pixel mask and speech bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of speech bubble data with keys 'x', 'y', 'w', 'h', 'text_ja', 'text_en'\n",
    "    margin : int\n",
    "        Extra margin to add around text regions\n",
    "    output_path : str, optional\n",
    "        Path to save the masked image\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        The masked image with text removed\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "    \n",
    "    # Create a drawing image for debugging\n",
    "    debug_img = img.copy()\n",
    "    draw = ImageDraw.Draw(debug_img)\n",
    "    \n",
    "    # Create output image\n",
    "    result_img = img_array.copy()\n",
    "    \n",
    "    # Process each bubble\n",
    "    for bubble in input_data:\n",
    "        x, y, w, h = bubble[\"x\"], bubble[\"y\"], bubble[\"w\"], bubble[\"h\"]\n",
    "        \n",
    "        # Draw original bubble outline\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=\"blue\", width=1)\n",
    "        \n",
    "        # Create a mask for this bubble region\n",
    "        bubble_region = np.zeros_like(mask)\n",
    "        bubble_region[y:y+h, x:x+w] = 1\n",
    "        \n",
    "        # Get the text mask within this bubble\n",
    "        text_in_bubble = np.logical_and(mask == 1, bubble_region == 1)\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(text_in_bubble) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find connected components (text clusters)\n",
    "        text_img = text_in_bubble.astype(np.uint8) * 255\n",
    "        \n",
    "        # Apply dilation to connect nearby text\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        dilated_text = cv2.dilate(text_img, kernel, iterations=1)\n",
    "        \n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_text, connectivity=8)\n",
    "        \n",
    "        # Process each text component\n",
    "        for i in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component bounding box\n",
    "            cx = stats[i, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[i, cv2.CC_STAT_TOP]\n",
    "            cw = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Expand with margin\n",
    "            cx_expanded = max(0, cx - margin)\n",
    "            cy_expanded = max(0, cy - margin)\n",
    "            cw_expanded = cw + 2 * margin\n",
    "            ch_expanded = ch + 2 * margin\n",
    "            \n",
    "            # Draw the expanded text region\n",
    "            draw.rectangle([cx_expanded, cy_expanded, cx_expanded+cw_expanded, cy_expanded+ch_expanded], \n",
    "                          outline=\"red\", width=1)\n",
    "            \n",
    "            # Create a mask for the expanded text region\n",
    "            text_region = np.zeros_like(mask)\n",
    "            text_region[cy_expanded:cy_expanded+ch_expanded, cx_expanded:cx_expanded+cw_expanded] = 1\n",
    "            \n",
    "            # Apply white color to the text region\n",
    "            result_img[text_region == 1] = [255, 255, 255]\n",
    "    \n",
    "    # Save debug image\n",
    "    debug_img.save(image_path.replace('.jpg', '_debug.jpg'))\n",
    "    \n",
    "    # Create output image\n",
    "    final_img = Image.fromarray(result_img)\n",
    "    \n",
    "    if output_path:\n",
    "        final_img.save(output_path)\n",
    "        print(f\"Masked image saved to: {output_path}\")\n",
    "    \n",
    "    return final_img\n",
    "\n",
    "def format_text_for_bubble(text, max_width, max_height, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    First try to wrap text, then consider adjusting font size.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(Image.new('RGB', (1, 1)))\n",
    "    font = ImageFont.truetype(font_path, base_size)\n",
    "    \n",
    "    # 1. First attempt simple text wrapping\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    \n",
    "    for word in words:\n",
    "        test_line = ' '.join(current_line + [word])\n",
    "        bbox = draw.textbbox((0, 0), test_line, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        \n",
    "        if text_width <= max_width:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            if current_line:\n",
    "                lines.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "            else:\n",
    "                # If the word is too long, consider breaking the word or adjusting the font size\n",
    "                current_line = [word]\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    \n",
    "    # 2. Check if the total height fits\n",
    "    total_height = 0\n",
    "    line_spacing = base_size * 0.3  # 30% line spacing\n",
    "    \n",
    "    for line in lines:\n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "\n",
    "    # 3. If height exceeds, consider reducing font size appropriately\n",
    "    if total_height > max_height:\n",
    "        # Reduce size by up to 20%\n",
    "        min_size = int(base_size * 0.8)\n",
    "        return format_text_for_bubble(text, max_width, max_height, font_path, min_size)\n",
    "    \n",
    "    return lines, base_size\n",
    "\n",
    "def add_translated_text(image, input_data, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    Add translated text to the image using the original bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : PIL.Image\n",
    "        The masked image\n",
    "    input_data : list of dict\n",
    "        List of bubble data with coordinates and translations\n",
    "    font_path : str\n",
    "        Path to the font file\n",
    "    base_size : int\n",
    "        Base font size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Image with translated text added\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Process each text area\n",
    "    for bubble in input_data:\n",
    "        # Get bubble coordinates and English text\n",
    "        x, y = bubble[\"x\"], bubble[\"y\"]\n",
    "        w, h = bubble[\"w\"], bubble[\"h\"]\n",
    "        text_en = bubble[\"text_en\"]\n",
    "        \n",
    "        # Format text to fit in the bubble\n",
    "        lines, final_size = format_text_for_bubble(\n",
    "            text_en,\n",
    "            w * 0.9,  # 90% of bubble width (10% margin)\n",
    "            h * 0.9,  # 90% of bubble height (10% margin)\n",
    "            font_path,\n",
    "            base_size\n",
    "        )\n",
    "        \n",
    "        # Load the font at the final determined size\n",
    "        font = ImageFont.truetype(font_path, final_size)\n",
    "        \n",
    "        # Calculate total text height (for vertical centering)\n",
    "        total_height = 0\n",
    "        line_spacing = final_size * 0.3\n",
    "        for line in lines:\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "        total_height -= line_spacing  # Subtract the line spacing added for the last line\n",
    "        \n",
    "        # Calculate the starting y-coordinate to vertically center the text\n",
    "        start_y = y + (h - total_height) // 2\n",
    "        \n",
    "        # Draw each line of text\n",
    "        current_y = start_y\n",
    "        for line in lines:\n",
    "            # Get the width of the current line for horizontal centering\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            text_width = bbox[2] - bbox[0]\n",
    "            text_x = x + (w - text_width) // 2\n",
    "            \n",
    "            # Draw text (with outline)\n",
    "            draw.text(\n",
    "                (text_x, current_y),\n",
    "                line,\n",
    "                font=font,\n",
    "                fill='black',\n",
    "                stroke_width=2,\n",
    "                stroke_fill='white'\n",
    "            )\n",
    "            \n",
    "            # Update the y-coordinate for the next line\n",
    "            current_y += (bbox[3] - bbox[1]) + line_spacing\n",
    "    \n",
    "    return image\n",
    "\n",
    "def translate_manga_page(image_path, mask_tensor, input_data, font_path, output_path=None, margin=5, base_font_size=22):\n",
    "    \"\"\"\n",
    "    Complete manga translation process:\n",
    "    1. Mask original text\n",
    "    2. Add translated text using original bubble coordinates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of bubble data with coordinates and translations\n",
    "    font_path : str\n",
    "        Path to the font file\n",
    "    output_path : str, optional\n",
    "        Path to save the translated image\n",
    "    margin : int\n",
    "        Margin to add around text regions for masking\n",
    "    base_font_size : int\n",
    "        Base font size for text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Translated image\n",
    "    \"\"\"\n",
    "    # Step 1: Mask the original text\n",
    "    masked_img = mask_text_with_bubbles(\n",
    "        image_path, mask_tensor, input_data, margin, \n",
    "        output_path=image_path.replace('.jpg', '_masked.jpg')\n",
    "    )\n",
    "    \n",
    "    # Step 2: Add translated text using original bubble coordinates\n",
    "    translated_img = add_translated_text(\n",
    "        masked_img, input_data, font_path, base_font_size\n",
    "    )\n",
    "    \n",
    "    # Save the translated image\n",
    "    if output_path:\n",
    "        translated_img.save(output_path)\n",
    "        print(f\"Translated image saved to: {output_path}\")\n",
    "    \n",
    "    return translated_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your exact variables\n",
    "    image_path = \"boureisougi_002.jpg\"\n",
    "    mask_dict = torch.load(f\"boureisougi_masks.pth\")\n",
    "    mask = mask_dict[\"boureisougi_002\"]\n",
    "    \n",
    "    # Your input data\n",
    "    input_data = [\n",
    "        {\"x\": 413, \"y\": 105, \"w\": 49, \"h\": 55, \"text_ja\": \"あ!\", \"text_en\": \"Oh!\"},\n",
    "        {\"x\": 492, \"y\": 236, \"w\": 100, \"h\": 154, \"text_ja\": \"あれは一丁目のスナックのママ!\", \"text_en\": \"That woman is the hostess in the bar at Block-1.\"},\n",
    "        {\"x\": 91, \"y\": 244, \"w\": 94, \"h\": 119, \"text_ja\": \"あっちは行きつけの店の女将!\", \"text_en\": \"That is the owner of my favorite restaurant!\"},\n",
    "        {\"x\": 625, \"y\": 457, \"w\": 89, \"h\": 120, \"text_ja\": \"ワシもまだまだ人気者ですなぁ!\", \"text_en\": \"I'm still so popular!\"},\n",
    "        {\"x\": 540, \"y\": 529, \"w\": 71, \"h\": 141, \"text_ja\": \"生き生きしますぞ!\", \"text_en\": \"I feel so alive!\"},\n",
    "        {\"x\": 565, \"y\": 701, \"w\": 54, \"h\": 96, \"text_ja\": \"葬儀屋とは\", \"text_en\": \"The job of an undertaker\"},\n",
    "        {\"x\": 150, \"y\": 704, \"w\": 78, \"h\": 112, \"text_ja\": \"生者と死者の最期の場所を作る仕事\", \"text_en\": \"is to set up the last place for the living and the dead.\"},\n",
    "        {\"x\": 701, \"y\": 916, \"w\": 62, \"h\": 68, \"text_ja\": \"..もう\", \"text_en\": \"Well, I'm afraid...\"},\n",
    "    ]\n",
    "    \n",
    "    # Font path (update this to match your system)\n",
    "    font_path = \"/System/Library/Fonts/Supplemental/Arial.ttf\"\n",
    "    \n",
    "    # Translate the manga page\n",
    "    output_path = \"boureisougi_002_translated.jpg\"\n",
    "    translated_img = translate_manga_page(\n",
    "        image_path, mask, input_data, font_path, \n",
    "        output_path=output_path, \n",
    "        margin=5, \n",
    "        base_font_size=22\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked image saved to: boureisougi_002_masked.jpg\n",
      "Translated image saved to: boureisougi_002_translated.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import cv2\n",
    "import platform\n",
    "import os\n",
    "\n",
    "def mask_text_with_bubbles(image_path, mask_tensor, input_data, margin=5, output_path=None):\n",
    "    \"\"\"\n",
    "    Mask text in manga using both pixel mask and speech bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of speech bubble data with keys 'x', 'y', 'w', 'h', 'text_ja', 'text_en'\n",
    "    margin : int\n",
    "        Extra margin to add around text regions\n",
    "    output_path : str, optional\n",
    "        Path to save the masked image\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        The masked image with text removed\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "    \n",
    "    # Create a drawing image for debugging\n",
    "    debug_img = img.copy()\n",
    "    draw = ImageDraw.Draw(debug_img)\n",
    "    \n",
    "    # Create output image\n",
    "    result_img = img_array.copy()\n",
    "    \n",
    "    # Process each bubble\n",
    "    for bubble in input_data:\n",
    "        x, y, w, h = bubble[\"x\"], bubble[\"y\"], bubble[\"w\"], bubble[\"h\"]\n",
    "        \n",
    "        # Draw original bubble outline\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=\"blue\", width=1)\n",
    "        \n",
    "        # Create a mask for this bubble region\n",
    "        bubble_region = np.zeros_like(mask)\n",
    "        bubble_region[y:y+h, x:x+w] = 1\n",
    "        \n",
    "        # Get the text mask within this bubble\n",
    "        text_in_bubble = np.logical_and(mask == 1, bubble_region == 1)\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(text_in_bubble) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find connected components (text clusters)\n",
    "        text_img = text_in_bubble.astype(np.uint8) * 255\n",
    "        \n",
    "        # Apply dilation to connect nearby text\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        dilated_text = cv2.dilate(text_img, kernel, iterations=1)\n",
    "        \n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_text, connectivity=8)\n",
    "        \n",
    "        # Process each text component\n",
    "        for i in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component bounding box\n",
    "            cx = stats[i, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[i, cv2.CC_STAT_TOP]\n",
    "            cw = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Expand with margin\n",
    "            cx_expanded = max(0, cx - margin)\n",
    "            cy_expanded = max(0, cy - margin)\n",
    "            cw_expanded = cw + 2 * margin\n",
    "            ch_expanded = ch + 2 * margin\n",
    "            \n",
    "            # Draw the expanded text region\n",
    "            draw.rectangle([cx_expanded, cy_expanded, cx_expanded+cw_expanded, cy_expanded+ch_expanded], \n",
    "                          outline=\"red\", width=1)\n",
    "            \n",
    "            # Create a mask for the expanded text region\n",
    "            text_region = np.zeros_like(mask)\n",
    "            text_region[cy_expanded:cy_expanded+ch_expanded, cx_expanded:cx_expanded+cw_expanded] = 1\n",
    "            \n",
    "            # Apply white color to the text region\n",
    "            result_img[text_region == 1] = [255, 255, 255]\n",
    "    \n",
    "    # Save debug image\n",
    "    debug_img.save(image_path.replace('.jpg', '_debug.jpg'))\n",
    "    \n",
    "    # Create output image\n",
    "    final_img = Image.fromarray(result_img)\n",
    "    \n",
    "    if output_path:\n",
    "        final_img.save(output_path)\n",
    "        print(f\"Masked image saved to: {output_path}\")\n",
    "    \n",
    "    return final_img\n",
    "\n",
    "def format_text_for_bubble(text, max_width, max_height, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    First try to wrap text, then consider adjusting font size.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(Image.new('RGB', (1, 1)))\n",
    "    font = ImageFont.truetype(font_path, base_size)\n",
    "    \n",
    "    # 1. First attempt simple text wrapping\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    \n",
    "    for word in words:\n",
    "        test_line = ' '.join(current_line + [word])\n",
    "        bbox = draw.textbbox((0, 0), test_line, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        \n",
    "        if text_width <= max_width:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            if current_line:\n",
    "                lines.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "            else:\n",
    "                # If the word is too long, consider breaking the word or adjusting the font size\n",
    "                current_line = [word]\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    \n",
    "    # 2. Check if the total height fits\n",
    "    total_height = 0\n",
    "    line_spacing = base_size * 0.3  # 30% line spacing\n",
    "    \n",
    "    for line in lines:\n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "\n",
    "    # 3. If height exceeds, consider reducing font size appropriately\n",
    "    if total_height > max_height:\n",
    "        # Reduce size by up to 20%\n",
    "        min_size = int(base_size * 0.8)\n",
    "        return format_text_for_bubble(text, max_width, max_height, font_path, min_size)\n",
    "    \n",
    "    return lines, base_size\n",
    "\n",
    "def add_translated_text(image, input_data, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    Add translated text to the image using the original bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : PIL.Image\n",
    "        The masked image\n",
    "    input_data : list of dict\n",
    "        List of bubble data with coordinates and translations\n",
    "    font_path : str\n",
    "        Path to the font file\n",
    "    base_size : int\n",
    "        Base font size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Image with translated text added\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Process each text area\n",
    "    for bubble in input_data:\n",
    "        # Get bubble coordinates and English text\n",
    "        x, y = bubble[\"x\"], bubble[\"y\"]\n",
    "        w, h = bubble[\"w\"], bubble[\"h\"]\n",
    "        text_en = bubble[\"text_en\"]\n",
    "        \n",
    "        # Format text to fit in the bubble\n",
    "        lines, final_size = format_text_for_bubble(\n",
    "            text_en,\n",
    "            w * 0.9,  # 90% of bubble width (10% margin)\n",
    "            h * 0.9,  # 90% of bubble height (10% margin)\n",
    "            font_path,\n",
    "            base_size\n",
    "        )\n",
    "        \n",
    "        # Load the font at the final determined size\n",
    "        font = ImageFont.truetype(font_path, final_size)\n",
    "        \n",
    "        # Calculate total text height (for vertical centering)\n",
    "        total_height = 0\n",
    "        line_spacing = final_size * 0.3\n",
    "        for line in lines:\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "        total_height -= line_spacing  # Subtract the line spacing added for the last line\n",
    "        \n",
    "        # Calculate the starting y-coordinate to vertically center the text\n",
    "        start_y = y + (h - total_height) // 2\n",
    "        \n",
    "        # Draw each line of text\n",
    "        current_y = start_y\n",
    "        for line in lines:\n",
    "            # Get the width of the current line for horizontal centering\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            text_width = bbox[2] - bbox[0]\n",
    "            text_x = x + (w - text_width) // 2\n",
    "            \n",
    "            # Draw text (with outline)\n",
    "            draw.text(\n",
    "                (text_x, current_y),\n",
    "                line,\n",
    "                font=font,\n",
    "                fill='black',\n",
    "                stroke_width=2,\n",
    "                stroke_fill='white'\n",
    "            )\n",
    "            \n",
    "            # Update the y-coordinate for the next line\n",
    "            current_y += (bbox[3] - bbox[1]) + line_spacing\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_system_font_path():\n",
    "    \"\"\"\n",
    "    Get appropriate font path based on the operating system.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to a system font\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Linux\":\n",
    "        font = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "    elif platform.system() == \"Darwin\":  # macOS\n",
    "        font = \"/System/Library/Fonts/Supplemental/Arial.ttf\"\n",
    "    elif platform.system() == \"Windows\":\n",
    "        font = \"C:/Windows/Fonts/arial.ttf\"\n",
    "    else:\n",
    "        # Fallback to a common location or raise an error\n",
    "        potential_paths = [\n",
    "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "            \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "            \"C:/Windows/Fonts/arial.ttf\"\n",
    "        ]\n",
    "        for path in potential_paths:\n",
    "            if os.path.exists(path):\n",
    "                font = path\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Could not find a suitable font on this system\")\n",
    "    \n",
    "    return font\n",
    "\n",
    "def translate_manga_page(image_path, mask_tensor, input_data, font_path=None, output_path=None, margin=5, base_font_size=22):\n",
    "    \"\"\"\n",
    "    Complete manga translation process:\n",
    "    1. Mask original text\n",
    "    2. Add translated text using original bubble coordinates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of bubble data with coordinates and translations\n",
    "    font_path : str\n",
    "        Path to the font file\n",
    "    output_path : str, optional\n",
    "        Path to save the translated image\n",
    "    margin : int\n",
    "        Margin to add around text regions for masking\n",
    "    base_font_size : int\n",
    "        Base font size for text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Translated image\n",
    "    \"\"\"\n",
    "    # Use system font path if none provided\n",
    "    if font_path is None:\n",
    "        font_path = get_system_font_path()\n",
    "    \n",
    "    # Step 1: Mask the original text\n",
    "    masked_img = mask_text_with_bubbles(\n",
    "        image_path, mask_tensor, input_data, margin, \n",
    "        output_path=image_path.replace('.jpg', '_masked.jpg')\n",
    "    )\n",
    "    \n",
    "    # Step 2: Add translated text using original bubble coordinates\n",
    "    translated_img = add_translated_text(\n",
    "        masked_img, input_data, font_path, base_font_size\n",
    "    )\n",
    "    \n",
    "    # Save the translated image\n",
    "    if output_path:\n",
    "        translated_img.save(output_path)\n",
    "        print(f\"Translated image saved to: {output_path}\")\n",
    "    \n",
    "    return translated_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your exact variables\n",
    "    image_path = \"boureisougi_002.jpg\"\n",
    "    mask_dict = torch.load(f\"boureisougi_masks.pth\")\n",
    "    mask = mask_dict[\"boureisougi_002\"]\n",
    "    \n",
    "    # Your input data\n",
    "    input_data = [\n",
    "        {\"x\": 413, \"y\": 105, \"w\": 49, \"h\": 55, \"text_ja\": \"あ!\", \"text_en\": \"Oh!\"},\n",
    "        {\"x\": 492, \"y\": 236, \"w\": 100, \"h\": 154, \"text_ja\": \"あれは一丁目のスナックのママ!\", \"text_en\": \"That woman is the hostess in the bar at Block-1.\"},\n",
    "        {\"x\": 91, \"y\": 244, \"w\": 94, \"h\": 119, \"text_ja\": \"あっちは行きつけの店の女将!\", \"text_en\": \"That is the owner of my favorite restaurant!\"},\n",
    "        {\"x\": 625, \"y\": 457, \"w\": 89, \"h\": 120, \"text_ja\": \"ワシもまだまだ人気者ですなぁ!\", \"text_en\": \"I'm still so popular!\"},\n",
    "        {\"x\": 540, \"y\": 529, \"w\": 71, \"h\": 141, \"text_ja\": \"生き生きしますぞ!\", \"text_en\": \"I feel so alive!\"},\n",
    "        {\"x\": 565, \"y\": 701, \"w\": 54, \"h\": 96, \"text_ja\": \"葬儀屋とは\", \"text_en\": \"The job of an undertaker\"},\n",
    "        {\"x\": 150, \"y\": 704, \"w\": 78, \"h\": 112, \"text_ja\": \"生者と死者の最期の場所を作る仕事\", \"text_en\": \"is to set up the last place for the living and the dead.\"},\n",
    "        {\"x\": 701, \"y\": 916, \"w\": 62, \"h\": 68, \"text_ja\": \"..もう\", \"text_en\": \"Well, I'm afraid...\"},\n",
    "    ]\n",
    "    \n",
    "    # Get platform-appropriate font path\n",
    "    font_path = get_system_font_path()\n",
    "    \n",
    "    # Translate the manga page\n",
    "    output_path = \"boureisougi_002_translated.jpg\"\n",
    "    translated_img = translate_manga_page(\n",
    "        image_path, mask, input_data, font_path, \n",
    "        output_path=output_path, \n",
    "        margin=5, \n",
    "        base_font_size=22\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with split long word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: pyphen library not found. Advanced hyphenation will not be available.\n",
      "To install: pip install pyphen\n",
      "Masked image saved to: boureisougi_002_masked.jpg\n",
      "Translated image saved to: boureisougi_002_translated.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import cv2\n",
    "import platform\n",
    "import os\n",
    "import re\n",
    "try:\n",
    "    import pyphen\n",
    "    PYPHEN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYPHEN_AVAILABLE = False\n",
    "    print(\"Warning: pyphen library not found. Advanced hyphenation will not be available.\")\n",
    "    print(\"To install: pip install pyphen\")\n",
    "\n",
    "def mask_text_with_bubbles(image_path, mask_tensor, input_data, margin=5, output_path=None):\n",
    "    \"\"\"\n",
    "    Mask text in manga using both pixel mask and speech bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of speech bubble data with keys 'x', 'y', 'w', 'h', 'text_ja', 'text_en'\n",
    "    margin : int\n",
    "        Extra margin to add around text regions\n",
    "    output_path : str, optional\n",
    "        Path to save the masked image\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        The masked image with text removed\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert mask tensor to numpy if needed\n",
    "    if isinstance(mask_tensor, torch.Tensor):\n",
    "        mask = mask_tensor.cpu().numpy()\n",
    "    else:\n",
    "        mask = mask_tensor\n",
    "    \n",
    "    # Create a drawing image for debugging\n",
    "    debug_img = img.copy()\n",
    "    draw = ImageDraw.Draw(debug_img)\n",
    "    \n",
    "    # Create output image\n",
    "    result_img = img_array.copy()\n",
    "    \n",
    "    # Process each bubble\n",
    "    for bubble in input_data:\n",
    "        x, y, w, h = bubble[\"x\"], bubble[\"y\"], bubble[\"w\"], bubble[\"h\"]\n",
    "        \n",
    "        # Draw original bubble outline\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=\"blue\", width=1)\n",
    "        \n",
    "        # Create a mask for this bubble region\n",
    "        bubble_region = np.zeros_like(mask)\n",
    "        bubble_region[y:y+h, x:x+w] = 1\n",
    "        \n",
    "        # Get the text mask within this bubble\n",
    "        text_in_bubble = np.logical_and(mask == 1, bubble_region == 1)\n",
    "        \n",
    "        # Skip if no text in this bubble\n",
    "        if np.sum(text_in_bubble) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find connected components (text clusters)\n",
    "        text_img = text_in_bubble.astype(np.uint8) * 255\n",
    "        \n",
    "        # Apply dilation to connect nearby text\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        dilated_text = cv2.dilate(text_img, kernel, iterations=1)\n",
    "        \n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated_text, connectivity=8)\n",
    "        \n",
    "        # Process each text component\n",
    "        for i in range(1, num_labels):  # Skip background (0)\n",
    "            # Get component bounding box\n",
    "            cx = stats[i, cv2.CC_STAT_LEFT]\n",
    "            cy = stats[i, cv2.CC_STAT_TOP]\n",
    "            cw = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            ch = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            # Expand with margin\n",
    "            cx_expanded = max(0, cx - margin)\n",
    "            cy_expanded = max(0, cy - margin)\n",
    "            cw_expanded = cw + 2 * margin\n",
    "            ch_expanded = ch + 2 * margin\n",
    "            \n",
    "            # Draw the expanded text region\n",
    "            draw.rectangle([cx_expanded, cy_expanded, cx_expanded+cw_expanded, cy_expanded+ch_expanded], \n",
    "                          outline=\"red\", width=1)\n",
    "            \n",
    "            # Create a mask for the expanded text region\n",
    "            text_region = np.zeros_like(mask)\n",
    "            text_region[cy_expanded:cy_expanded+ch_expanded, cx_expanded:cx_expanded+cw_expanded] = 1\n",
    "            \n",
    "            # Apply white color to the text region\n",
    "            result_img[text_region == 1] = [255, 255, 255]\n",
    "    \n",
    "    # Save debug image\n",
    "    debug_img.save(image_path.replace('.jpg', '_debug.jpg'))\n",
    "    \n",
    "    # Create output image\n",
    "    final_img = Image.fromarray(result_img)\n",
    "    \n",
    "    if output_path:\n",
    "        final_img.save(output_path)\n",
    "        print(f\"Masked image saved to: {output_path}\")\n",
    "    \n",
    "    return final_img\n",
    "\n",
    "def hyphenate_word(word, dic=None):\n",
    "    \"\"\"\n",
    "    Hyphenate a word using pyphen if available, otherwise use a simple rule-based approach.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    word : str\n",
    "        Word to hyphenate\n",
    "    dic : pyphen.Pyphen, optional\n",
    "        Pyphen dictionary for language-specific hyphenation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of syllables that can be joined with hyphens\n",
    "    \"\"\"\n",
    "    # If word is short, no need to hyphenate\n",
    "    if len(word) <= 6:\n",
    "        return [word]\n",
    "    \n",
    "    # Use pyphen if available for better hyphenation\n",
    "    if PYPHEN_AVAILABLE and dic is not None:\n",
    "        # Get hyphenation points using pyphen\n",
    "        parts = dic.inserted(word, hyphen='-').split('-')\n",
    "        return parts\n",
    "    \n",
    "    # Simple fallback hyphenation method\n",
    "    # Try to split at reasonable points for English words\n",
    "    syllables = []\n",
    "    remaining = word\n",
    "    \n",
    "    # Common English prefixes\n",
    "    prefixes = ['un', 'in', 're', 'dis', 'over', 'under', 'pre', 'post', 'non', 'anti']\n",
    "    # Common English suffixes\n",
    "    suffixes = ['ing', 'tion', 'ment', 'ness', 'able', 'ible', 'ful', 'less', 'ize', 'ise']\n",
    "    \n",
    "    # Check for prefixes\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix) and len(word) > len(prefix) + 3:\n",
    "            syllables.append(prefix)\n",
    "            remaining = word[len(prefix):]\n",
    "            break\n",
    "    \n",
    "    # Check for suffixes in what's left\n",
    "    for suffix in suffixes:\n",
    "        if remaining.endswith(suffix) and len(remaining) > len(suffix) + 3:\n",
    "            end_part = remaining[-len(suffix):]\n",
    "            remaining = remaining[:-len(suffix)]\n",
    "            \n",
    "            # Simple approach: split the middle part in half if it's long enough\n",
    "            if len(remaining) > 6:\n",
    "                mid_point = len(remaining) // 2\n",
    "                syllables.append(remaining[:mid_point])\n",
    "                syllables.append(remaining[mid_point:])\n",
    "            else:\n",
    "                syllables.append(remaining)\n",
    "                \n",
    "            syllables.append(end_part)\n",
    "            return syllables\n",
    "    \n",
    "    # If no prefix/suffix matched or remaining part is still long, use a simple splitting approach\n",
    "    if len(remaining) > 8:\n",
    "        # Try to split at vowel-consonant boundaries\n",
    "        vowels = 'aeiou'\n",
    "        parts = []\n",
    "        temp = \"\"\n",
    "        \n",
    "        for i in range(len(remaining)):\n",
    "            temp += remaining[i]\n",
    "            \n",
    "            # Look for vowel followed by consonant as potential break points\n",
    "            if (i > 0 and i < len(remaining) - 2 and \n",
    "                remaining[i-1].lower() in vowels and \n",
    "                remaining[i].lower() not in vowels):\n",
    "                \n",
    "                if len(temp) >= 3:  # Ensure each part is at least 3 chars\n",
    "                    parts.append(temp)\n",
    "                    temp = \"\"\n",
    "        \n",
    "        if temp:  # Add any remaining part\n",
    "            parts.append(temp)\n",
    "            \n",
    "        if len(parts) > 1:\n",
    "            return parts\n",
    "        \n",
    "        # If vowel-consonant approach didn't work well, fall back to simpler approach\n",
    "        mid_point = len(remaining) // 2\n",
    "        return [remaining[:mid_point], remaining[mid_point:]]\n",
    "    \n",
    "    # If word is not that long or we couldn't split it well, return as is\n",
    "    return [remaining]\n",
    "\n",
    "def format_text_for_bubble(text, max_width, max_height, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    Format text to fit within a bubble with line wrapping, hyphenation for long words,\n",
    "    and font size adjustment if needed.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(Image.new('RGB', (1, 1)))\n",
    "    font = ImageFont.truetype(font_path, base_size)\n",
    "    \n",
    "    # Initialize hyphenation dictionary if pyphen is available\n",
    "    if PYPHEN_AVAILABLE:\n",
    "        dic = pyphen.Pyphen(lang='en_US')\n",
    "    else:\n",
    "        dic = None\n",
    "    \n",
    "    # 1. First attempt text wrapping with hyphenation for long words\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Test if adding this word exceeds the width\n",
    "        test_line = ' '.join(current_line + [word])\n",
    "        bbox = draw.textbbox((0, 0), test_line, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        \n",
    "        if text_width <= max_width:\n",
    "            # Word fits, add it to current line\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            if current_line:\n",
    "                # Add the current line to lines and start a new line\n",
    "                lines.append(' '.join(current_line))\n",
    "                \n",
    "                # Start new line with current word\n",
    "                current_line = [word]\n",
    "                \n",
    "                # Check if this word alone exceeds the width (needs hyphenation)\n",
    "                bbox = draw.textbbox((0, 0), word, font=font)\n",
    "                if bbox[2] - bbox[0] > max_width:\n",
    "                    # Word is too long, need to hyphenate\n",
    "                    current_line = []\n",
    "                    syllables = hyphenate_word(word, dic)\n",
    "                    \n",
    "                    # Build line with hyphenated parts\n",
    "                    temp_line = \"\"\n",
    "                    for i, part in enumerate(syllables):\n",
    "                        test_part = temp_line + part\n",
    "                        \n",
    "                        # If not the last part, add hyphen for testing\n",
    "                        if i < len(syllables) - 1:\n",
    "                            test_part += \"-\"\n",
    "                            \n",
    "                        # Test width\n",
    "                        bbox = draw.textbbox((0, 0), test_part, font=font)\n",
    "                        if bbox[2] - bbox[0] <= max_width:\n",
    "                            temp_line = test_part\n",
    "                        else:\n",
    "                            # This part doesn't fit, add what we have so far\n",
    "                            if temp_line:\n",
    "                                lines.append(temp_line)\n",
    "                                temp_line = part\n",
    "                                # Add hyphen if not the last part\n",
    "                                if i < len(syllables) - 1:\n",
    "                                    temp_line += \"-\"\n",
    "                            else:\n",
    "                                # Forced break within a syllable (rare case)\n",
    "                                syllable_len = len(part)\n",
    "                                mid = syllable_len // 2\n",
    "                                lines.append(part[:mid] + \"-\")\n",
    "                                temp_line = part[mid:]\n",
    "                                # Add hyphen if not the last part\n",
    "                                if i < len(syllables) - 1:\n",
    "                                    temp_line += \"-\"\n",
    "                    \n",
    "                    # Add any remaining parts\n",
    "                    if temp_line:\n",
    "                        current_line = [temp_line]\n",
    "            else:\n",
    "                # First word on the line is already too long\n",
    "                syllables = hyphenate_word(word, dic)\n",
    "                \n",
    "                # Try to fit syllables\n",
    "                temp_line = \"\"\n",
    "                for i, part in enumerate(syllables):\n",
    "                    test_part = temp_line + part\n",
    "                    \n",
    "                    # If not the last part, add hyphen for testing\n",
    "                    if i < len(syllables) - 1:\n",
    "                        test_part += \"-\"\n",
    "                        \n",
    "                    # Test width\n",
    "                    bbox = draw.textbbox((0, 0), test_part, font=font)\n",
    "                    if bbox[2] - bbox[0] <= max_width:\n",
    "                        temp_line = test_part\n",
    "                    else:\n",
    "                        # This part doesn't fit, add what we have so far\n",
    "                        if temp_line:\n",
    "                            lines.append(temp_line)\n",
    "                            temp_line = part\n",
    "                            # Add hyphen if not the last part\n",
    "                            if i < len(syllables) - 1:\n",
    "                                temp_line += \"-\"\n",
    "                        else:\n",
    "                            # Forced break within a syllable (rare case)\n",
    "                            syllable_len = len(part)\n",
    "                            mid = syllable_len // 2\n",
    "                            lines.append(part[:mid] + \"-\")\n",
    "                            temp_line = part[mid:]\n",
    "                            # Add hyphen if not the last part\n",
    "                            if i < len(syllables) - 1:\n",
    "                                temp_line += \"-\"\n",
    "                \n",
    "                # Add any remaining parts\n",
    "                if temp_line:\n",
    "                    current_line = [temp_line]\n",
    "    \n",
    "    # Add the last line if there's anything left\n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    \n",
    "    # 2. Check if the total height fits\n",
    "    total_height = 0\n",
    "    line_spacing = base_size * 0.3  # 30% line spacing\n",
    "    \n",
    "    for line in lines:\n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "\n",
    "    # 3. If height exceeds, consider reducing font size appropriately\n",
    "    if total_height > max_height:\n",
    "        # Calculate new size based on how much we exceed\n",
    "        ratio = max_height / total_height\n",
    "        new_size = max(int(base_size * ratio * 0.95), 10)  # Minimum size of 10\n",
    "        \n",
    "        # Recursively try with smaller font\n",
    "        return format_text_for_bubble(text, max_width, max_height, font_path, new_size)\n",
    "    \n",
    "    return lines, base_size\n",
    "\n",
    "def add_translated_text(image, input_data, font_path, base_size=22):\n",
    "    \"\"\"\n",
    "    Add translated text to the image using the original bubble coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : PIL.Image\n",
    "        The masked image\n",
    "    input_data : list of dict\n",
    "        List of bubble data with coordinates and translations\n",
    "    font_path : str\n",
    "        Path to the font file\n",
    "    base_size : int\n",
    "        Base font size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Image with translated text added\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Process each text area\n",
    "    for bubble in input_data:\n",
    "        # Get bubble coordinates and English text\n",
    "        x, y = bubble[\"x\"], bubble[\"y\"]\n",
    "        w, h = bubble[\"w\"], bubble[\"h\"]\n",
    "        text_en = bubble[\"text_en\"]\n",
    "        \n",
    "        # Format text to fit in the bubble\n",
    "        lines, final_size = format_text_for_bubble(\n",
    "            text_en,\n",
    "            w * 0.9,  # 90% of bubble width (10% margin)\n",
    "            h * 0.9,  # 90% of bubble height (10% margin)\n",
    "            font_path,\n",
    "            base_size\n",
    "        )\n",
    "        \n",
    "        # Load the font at the final determined size\n",
    "        font = ImageFont.truetype(font_path, final_size)\n",
    "        \n",
    "        # Calculate total text height (for vertical centering)\n",
    "        total_height = 0\n",
    "        line_spacing = final_size * 0.3\n",
    "        for line in lines:\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            total_height += (bbox[3] - bbox[1]) + line_spacing\n",
    "        total_height -= line_spacing  # Subtract the line spacing added for the last line\n",
    "        \n",
    "        # Calculate the starting y-coordinate to vertically center the text\n",
    "        start_y = y + (h - total_height) // 2\n",
    "        \n",
    "        # Draw each line of text\n",
    "        current_y = start_y\n",
    "        for line in lines:\n",
    "            # Get the width of the current line for horizontal centering\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            text_width = bbox[2] - bbox[0]\n",
    "            text_x = x + (w - text_width) // 2\n",
    "            \n",
    "            # Draw text (with outline)\n",
    "            draw.text(\n",
    "                (text_x, current_y),\n",
    "                line,\n",
    "                font=font,\n",
    "                fill='black',\n",
    "                stroke_width=2,\n",
    "                stroke_fill='white'\n",
    "            )\n",
    "            \n",
    "            # Update the y-coordinate for the next line\n",
    "            current_y += (bbox[3] - bbox[1]) + line_spacing\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_system_font_path():\n",
    "    \"\"\"\n",
    "    Get appropriate font path based on the operating system.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to a system font\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Linux\":\n",
    "        font = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "    elif platform.system() == \"Darwin\":  # macOS\n",
    "        font = \"/System/Library/Fonts/Supplemental/Arial.ttf\"\n",
    "    elif platform.system() == \"Windows\":\n",
    "        font = \"C:/Windows/Fonts/arial.ttf\"\n",
    "    else:\n",
    "        # Fallback to a common location or raise an error\n",
    "        potential_paths = [\n",
    "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "            \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "            \"C:/Windows/Fonts/arial.ttf\"\n",
    "        ]\n",
    "        for path in potential_paths:\n",
    "            if os.path.exists(path):\n",
    "                font = path\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Could not find a suitable font on this system\")\n",
    "    \n",
    "    return font\n",
    "\n",
    "def translate_manga_page(image_path, mask_tensor, input_data, font_path=None, output_path=None, margin=5, base_font_size=22):\n",
    "    \"\"\"\n",
    "    Complete manga translation process:\n",
    "    1. Mask original text\n",
    "    2. Add translated text using original bubble coordinates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the manga image\n",
    "    mask_tensor : torch.Tensor\n",
    "        Binary mask tensor where 1 indicates text pixels\n",
    "    input_data : list of dict\n",
    "        List of bubble data with coordinates and translations\n",
    "    font_path : str\n",
    "        Path to the font file\n",
    "    output_path : str, optional\n",
    "        Path to save the translated image\n",
    "    margin : int\n",
    "        Margin to add around text regions for masking\n",
    "    base_font_size : int\n",
    "        Base font size for text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PIL.Image\n",
    "        Translated image\n",
    "    \"\"\"\n",
    "    # Use system font path if none provided\n",
    "    if font_path is None:\n",
    "        font_path = get_system_font_path()\n",
    "    \n",
    "    # Step 1: Mask the original text\n",
    "    masked_img = mask_text_with_bubbles(\n",
    "        image_path, mask_tensor, input_data, margin, \n",
    "        output_path=image_path.replace('.jpg', '_masked.jpg')\n",
    "    )\n",
    "    \n",
    "    # Step 2: Add translated text using original bubble coordinates\n",
    "    translated_img = add_translated_text(\n",
    "        masked_img, input_data, font_path, base_font_size\n",
    "    )\n",
    "    \n",
    "    # Save the translated image\n",
    "    if output_path:\n",
    "        translated_img.save(output_path)\n",
    "        print(f\"Translated image saved to: {output_path}\")\n",
    "    \n",
    "    return translated_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your exact variables\n",
    "    image_path = \"boureisougi_002.jpg\"\n",
    "    mask_dict = torch.load(f\"boureisougi_masks.pth\")\n",
    "    mask = mask_dict[\"boureisougi_002\"]\n",
    "    \n",
    "    # Your input data\n",
    "    input_data = [\n",
    "        {\"x\": 413, \"y\": 105, \"w\": 49, \"h\": 55, \"text_ja\": \"あ!\", \"text_en\": \"Oh!\"},\n",
    "        {\"x\": 492, \"y\": 236, \"w\": 100, \"h\": 154, \"text_ja\": \"あれは一丁目のスナックのママ!\", \"text_en\": \"That woman is the hostess in the bar at Block-1.\"},\n",
    "        {\"x\": 91, \"y\": 244, \"w\": 94, \"h\": 119, \"text_ja\": \"あっちは行きつけの店の女将!\", \"text_en\": \"That is the owner of my favorite restaurant!\"},\n",
    "        {\"x\": 625, \"y\": 457, \"w\": 89, \"h\": 120, \"text_ja\": \"ワシもまだまだ人気者ですなぁ!\", \"text_en\": \"I'm still so popular!\"},\n",
    "        {\"x\": 540, \"y\": 529, \"w\": 71, \"h\": 141, \"text_ja\": \"生き生きしますぞ!\", \"text_en\": \"I feel so alive!\"},\n",
    "        {\"x\": 565, \"y\": 701, \"w\": 54, \"h\": 96, \"text_ja\": \"葬儀屋とは\", \"text_en\": \"The job of an undertaker\"},\n",
    "        {\"x\": 150, \"y\": 704, \"w\": 78, \"h\": 112, \"text_ja\": \"生者と死者の最期の場所を作る仕事\", \"text_en\": \"is to set up the last place for the living and the dead.\"},\n",
    "        {\"x\": 701, \"y\": 916, \"w\": 62, \"h\": 68, \"text_ja\": \"..もう\", \"text_en\": \"Well, I'm afraid...\"},\n",
    "    ]\n",
    "    \n",
    "    # Get platform-appropriate font path\n",
    "    font_path = get_system_font_path()\n",
    "    \n",
    "    # Translate the manga page\n",
    "    output_path = \"boureisougi_002_translated.jpg\"\n",
    "    translated_img = translate_manga_page(\n",
    "        image_path, mask, input_data, font_path, \n",
    "        output_path=output_path, \n",
    "        margin=5, \n",
    "        base_font_size=22\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t_ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
